# 格式转换 (safetensor -> gguf)

终端 Powershell
**创建虚拟环境**
conda create -n gguf python=3.10 -y
conda activate gguf
结果：(gguf) C:\Users\xxxxx> <br>

**下载hf（安装 HuggingFace CLI命令行工具）**
powershell -ExecutionPolicy ByPass -c "irm https://hf.co/cli/install.ps1 | iex"
**下载模型**
hf download Qwen/Qwen3-1.7B 
  --local-dir D:\llama\models\qwen3-1.7b 
  --resume
local-dir：指定下载目录
resume：支持断点续传 <br>

**下载llama.cpp(含convert_hf_to_gguf.py)**
下载整个仓库zip：https://github.com/ggerganov/llama.cpp/archive/refs/heads/master.zip
解压到：D:\llama\llama_src\
安装依赖：
pip install transformers sentencepiece accelerate protobuf
pip uninstall -y gguf     # 必须卸载，否则冲突 <br>

**转换f16格式**
转换脚本位置：D:\llama\llama_src\convert_hf_to_gguf.py
python D:\llama\llama_src\convert_hf_to_gguf.py 
  D:\llama\models\qwen3-1.7b 
  --outfile D:\llama\models\qwen3-1.7b-f16.gguf
完成后会生成：qwen3-1.7b-f16.gguf <br>

**开始量化**
量化工具位置：D:\llama\llama.cpp\llama-quantize.exe
D:\llama\llama.cpp\llama-quantize.exe 
  D:\llama\llama_src\qwen3-1.7b-f16.gguf 
  D:\llama\llama_src\qwen3-1.7b-q5_k_m.gguf `
  q5_k_m
最后一个参数（q4_k_m, q5_k_m, q8_0）决定量化格式
完成后会生成：qwen3-1.7b-q4_k_m.gguf <br>

**测试**
D:\llama\llama.cpp\llama-cli.exe 
  -m D:\llama\llama_src\qwen3-1.7b-q4_k_m.gguf 
  -p "who are you" <br>

**补充：**
常见格式：q5_k_m、q4_k_s、q2_k、q8_0
k：K-block（精度更好）
M（Mixed）：混合量化（比 S 精度更高）S（Symmetric）：对称量化


